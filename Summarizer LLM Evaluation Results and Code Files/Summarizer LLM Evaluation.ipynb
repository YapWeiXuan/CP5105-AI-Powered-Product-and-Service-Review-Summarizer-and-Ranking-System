{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from codebleu import calc_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from datasets import Dataset \n",
    "from ragas.metrics import summarization_score\n",
    "from ragas import evaluate\n",
    "import import_ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counter and Generate Summary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(text):\n",
    "    \n",
    "    counter = len(text.split())\n",
    "    \n",
    "    return counter\n",
    "\n",
    "def generate_summary(text, model):\n",
    "    \n",
    "    summary_word_limit = int(0.2*word_counter(text))\n",
    "    \n",
    "    prompt =  f\"Summarize the given text in maximum {summary_word_limit} words. \\\n",
    "               Extract the most important information. \\\n",
    "               Only output the summary without any additional text.\"\n",
    "    \n",
    "    response = ollama.chat(model=model, messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': text,\n",
    "        },\n",
    "    ])\n",
    "    \n",
    "    summary = response['message']['content']\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading sujayC66/text_summarization_512_length_1_4000 dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_set = load_dataset(\"sujayC66/text_summarization_512_length_1_4000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>model_summary</th>\n",
       "      <th>original_count</th>\n",
       "      <th>model_count</th>\n",
       "      <th>pct_model_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON - Hunting PLC (LSE: HTG), a precision e...</td>\n",
       "      <td>Hunting PLC's 2023 financial performance align...</td>\n",
       "      <td>410</td>\n",
       "      <td>54</td>\n",
       "      <td>13.170732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Promoter entity of Sapphire Foods India Arinja...</td>\n",
       "      <td>Sapphire Foods India's promoter, Arinjaya (Mau...</td>\n",
       "      <td>317</td>\n",
       "      <td>47</td>\n",
       "      <td>14.826498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gold price climbed Rs 410 to Rs 61,210 per 10 ...</td>\n",
       "      <td>Gold prices rose by Rs 410 to Rs 61,210 per 10...</td>\n",
       "      <td>325</td>\n",
       "      <td>54</td>\n",
       "      <td>16.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Delhi, Jan 11 (IANS)  Life Insurance  Corp...</td>\n",
       "      <td>LIC received orders for Rs 3,528 crore from In...</td>\n",
       "      <td>218</td>\n",
       "      <td>60</td>\n",
       "      <td>27.522936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“I don’t want to sound alarmist, but it pays t...</td>\n",
       "      <td>MFIs have become the largest providers of micr...</td>\n",
       "      <td>93</td>\n",
       "      <td>29</td>\n",
       "      <td>31.182796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  LONDON - Hunting PLC (LSE: HTG), a precision e...   \n",
       "1  Promoter entity of Sapphire Foods India Arinja...   \n",
       "2  Gold price climbed Rs 410 to Rs 61,210 per 10 ...   \n",
       "3  New Delhi, Jan 11 (IANS)  Life Insurance  Corp...   \n",
       "4  “I don’t want to sound alarmist, but it pays t...   \n",
       "\n",
       "                                       model_summary  original_count  \\\n",
       "0  Hunting PLC's 2023 financial performance align...             410   \n",
       "1  Sapphire Foods India's promoter, Arinjaya (Mau...             317   \n",
       "2  Gold prices rose by Rs 410 to Rs 61,210 per 10...             325   \n",
       "3  LIC received orders for Rs 3,528 crore from In...             218   \n",
       "4  MFIs have become the largest providers of micr...              93   \n",
       "\n",
       "   model_count  pct_model_count  \n",
       "0           54        13.170732  \n",
       "1           47        14.826498  \n",
       "2           54        16.615385  \n",
       "3           60        27.522936  \n",
       "4           29        31.182796  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(raw_data_set['train'])\n",
    "df=df.rename(columns={\"content\":\"text\",\"summary\":\"model_summary\"})\n",
    "train_df = df.drop(columns = [ \"__index_level_0__\"])\n",
    "train_df['original_count'] = train_df['text'].apply(word_counter)\n",
    "train_df['model_count'] = train_df['model_summary'].apply(word_counter)\n",
    "train_df['pct_model_count'] = 100*(train_df['model_count']/train_df['original_count'])\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3377, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= train_df[:3200]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM Models to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Models \n",
    "\n",
    "llm_models = [\"phi3:latest\", \"deepseek-llm:latest\", \"mistral:latest\", \"llama3.1:latest\",  \"qwen2:latest\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Summaries for each LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi3:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\1445589656.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[llm + \"_GenSummary\"] = df[\"text\"].apply(lambda x: generate_summary(x, model=llm))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-llm:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\1445589656.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[llm + \"_GenSummary\"] = df[\"text\"].apply(lambda x: generate_summary(x, model=llm))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\1445589656.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[llm + \"_GenSummary\"] = df[\"text\"].apply(lambda x: generate_summary(x, model=llm))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.1:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\1445589656.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[llm + \"_GenSummary\"] = df[\"text\"].apply(lambda x: generate_summary(x, model=llm))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen2:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\1445589656.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[llm + \"_GenSummary\"] = df[\"text\"].apply(lambda x: generate_summary(x, model=llm))\n"
     ]
    }
   ],
   "source": [
    "df= train_df[:3200]\n",
    "\n",
    "for llm in llm_models: \n",
    "    print(llm)\n",
    "    df[llm + \"_GenSummary\"] = df[\"text\"].apply(lambda x: generate_summary(x, model=llm))\n",
    "    df.to_csv(f'test_3200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'test_3200_backup.csv')\n",
    "df.to_excel(f'test_3200.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 15)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Word Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\2389567114.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pct_count] = df[column_name].apply(word_counter)\n",
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\2389567114.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pct_count] = 100*df[pct_count]/df['original_count']\n",
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\2389567114.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pct_count] = df[column_name].apply(word_counter)\n",
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\2389567114.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pct_count] = 100*df[pct_count]/df['original_count']\n",
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\2389567114.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pct_count] = df[column_name].apply(word_counter)\n",
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\2389567114.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pct_count] = 100*df[pct_count]/df['original_count']\n",
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\2389567114.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pct_count] = df[column_name].apply(word_counter)\n",
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\2389567114.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pct_count] = 100*df[pct_count]/df['original_count']\n",
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\2389567114.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pct_count] = df[column_name].apply(word_counter)\n",
      "C:\\Users\\flame\\AppData\\Local\\Temp\\ipykernel_17116\\2389567114.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pct_count] = 100*df[pct_count]/df['original_count']\n"
     ]
    }
   ],
   "source": [
    "#Get word_count_pct\n",
    "for llm in llm_models: \n",
    "    column_name = llm + \"_GenSummary\"\n",
    "    pct_count = llm + \"_pct_count\"\n",
    "    df[pct_count] = df[column_name].apply(word_counter)\n",
    "    df[pct_count] = 100*df[pct_count]/df['original_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_df = pd.DataFrame()\n",
    "word_count_df['phi'] = df['phi3:latest_pct_count']\n",
    "word_count_df['deepseek'] = df['deepseek-llm:latest_pct_count']\n",
    "word_count_df['mistral'] = df['mistral:latest_pct_count']\n",
    "word_count_df['llama3.1'] = df['llama3.1:latest_pct_count']\n",
    "word_count_df['qwen2'] = df['qwen2:latest_pct_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'model_summary', 'original_count', 'model_count',\n",
       "       'pct_model_count', 'phi3:latest_GenSummary',\n",
       "       'deepseek-llm:latest_GenSummary', 'mistral:latest_GenSummary',\n",
       "       'llama3.1:latest_GenSummary', 'qwen2:latest_GenSummary',\n",
       "       'phi3:latest_pct_count', 'deepseek-llm:latest_pct_count',\n",
       "       'mistral:latest_pct_count', 'llama3.1:latest_pct_count',\n",
       "       'qwen2:latest_pct_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phi</th>\n",
       "      <th>deepseek</th>\n",
       "      <th>mistral</th>\n",
       "      <th>llama3.1</th>\n",
       "      <th>qwen2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3200.000000</td>\n",
       "      <td>3200.000000</td>\n",
       "      <td>3200.000000</td>\n",
       "      <td>3200.000000</td>\n",
       "      <td>3200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.454865</td>\n",
       "      <td>16.566206</td>\n",
       "      <td>29.886796</td>\n",
       "      <td>15.218574</td>\n",
       "      <td>31.795555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>62.629530</td>\n",
       "      <td>7.269763</td>\n",
       "      <td>9.135821</td>\n",
       "      <td>2.112653</td>\n",
       "      <td>11.828262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.908277</td>\n",
       "      <td>10.454545</td>\n",
       "      <td>6.983240</td>\n",
       "      <td>0.273224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.439024</td>\n",
       "      <td>11.519341</td>\n",
       "      <td>23.387378</td>\n",
       "      <td>13.760698</td>\n",
       "      <td>24.091205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.481654</td>\n",
       "      <td>15.411912</td>\n",
       "      <td>28.312957</td>\n",
       "      <td>15.147873</td>\n",
       "      <td>30.285428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.666667</td>\n",
       "      <td>20.263646</td>\n",
       "      <td>34.554924</td>\n",
       "      <td>16.568047</td>\n",
       "      <td>37.698943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3184.042553</td>\n",
       "      <td>67.307692</td>\n",
       "      <td>74.489796</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>168.518519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               phi     deepseek      mistral     llama3.1        qwen2\n",
       "count  3200.000000  3200.000000  3200.000000  3200.000000  3200.000000\n",
       "mean     14.454865    16.566206    29.886796    15.218574    31.795555\n",
       "std      62.629530     7.269763     9.135821     2.112653    11.828262\n",
       "min       0.000000     2.908277    10.454545     6.983240     0.273224\n",
       "25%       2.439024    11.519341    23.387378    13.760698    24.091205\n",
       "50%       7.481654    15.411912    28.312957    15.147873    30.285428\n",
       "75%      16.666667    20.263646    34.554924    16.568047    37.698943\n",
       "max    3184.042553    67.307692    74.489796    25.000000   168.518519"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import Eval_Metrics\n",
    "import numpy as np\n",
    "eval_metrics = Eval_Metrics.Evaluation_Metrics()\n",
    "Eval_df_copy =df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(f'test_3200_FINAL_backup.csv')\n",
    "df.to_excel(f'test_3200_FINAL.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi3:latest\n",
      "{'rouge-1': {'r': 0.07559622704413912, 'p': 0.09085900441705777, 'f': 0.07202600748898173}, 'rouge-2': {'r': 0.010513045440923866, 'p': 0.009372265398385074, 'f': 0.009198374266425939}, 'rouge-l': {'r': 0.06615216251378056, 'p': 0.08153424272049929, 'f': 0.06327465564639882}}\n",
      "deepseek-llm:latest\n",
      "{'rouge-1': {'r': 0.3984040852265913, 'p': 0.4504441678098226, 'f': 0.4019042115689887}, 'rouge-2': {'r': 0.18189963691944783, 'p': 0.21264128206886695, 'f': 0.18479483180486755}, 'rouge-l': {'r': 0.36103406136402055, 'p': 0.40811292894938683, 'f': 0.36398686011750464}}\n",
      "mistral:latest\n",
      "{'rouge-1': {'r': 0.5675780646734188, 'p': 0.38540428072344257, 'f': 0.4446109864414285}, 'rouge-2': {'r': 0.2881548902176165, 'p': 0.19232923667430415, 'f': 0.2217566776633234}, 'rouge-l': {'r': 0.5254460893796375, 'p': 0.35623218266650397, 'f': 0.4112549076362883}}\n",
      "llama3.1:latest\n",
      "{'rouge-1': {'r': 0.46811666903497645, 'p': 0.5448599500066181, 'f': 0.48796842803339524}, 'rouge-2': {'r': 0.25303461739655697, 'p': 0.3114680833486883, 'f': 0.269300499628222}, 'rouge-l': {'r': 0.43017824994396664, 'p': 0.5005244825709253, 'f': 0.448394900172394}}\n",
      "qwen2:latest\n",
      "{'rouge-1': {'r': 0.5838851862248409, 'p': 0.38099788407036633, 'f': 0.4396014677469355}, 'rouge-2': {'r': 0.2939579450331128, 'p': 0.1886563559536737, 'f': 0.217141856920901}, 'rouge-l': {'r': 0.5363203180642534, 'p': 0.34928156124364923, 'f': 0.40329162508452937}}\n"
     ]
    }
   ],
   "source": [
    "## Rouge Scores\n",
    "AllRouge_Scores = pd.DataFrame()\n",
    "AllRouge_Scores = pd.DataFrame(columns=['LLM_Model','rouge-1-r','rouge-1-p','rouge-1-f','rouge-2-r','rouge-2-p','rouge-2-f','rouge-l-r','rouge-l-p','rouge-l-f'])\n",
    "\n",
    "# llm_model = ['phi3:latest','llama3.1:latest']\n",
    "for llm in llm_models: \n",
    "# for llm in llm_model: \n",
    "    print(llm)\n",
    "    GeneratedSummary_col = llm + \"_GenSummary\"\n",
    "    ModelSummary_col = 'model_summary'\n",
    "    \n",
    "    test_rouge_df = Eval_df_copy[[GeneratedSummary_col,ModelSummary_col]].copy(deep=True)\n",
    "    test_rouge_df[GeneratedSummary_col] = test_rouge_df[GeneratedSummary_col].replace('', np.nan)\n",
    "    test_rouge_df = test_rouge_df.dropna(subset=[GeneratedSummary_col])\n",
    "    test_rouge_df = test_rouge_df.reset_index(drop=True)\n",
    "    \n",
    "    rouge_scores = eval_metrics.get_rouge_scores(test_rouge_df[ModelSummary_col],test_rouge_df[GeneratedSummary_col])\n",
    "    \n",
    "    print(rouge_scores)\n",
    "    ref_row = [llm , \n",
    "               rouge_scores['rouge-1']['r'] , \n",
    "               rouge_scores['rouge-1']['p'], \n",
    "               rouge_scores['rouge-1']['f'], \n",
    "               rouge_scores['rouge-2']['r'], \n",
    "               rouge_scores['rouge-2']['p'], \n",
    "               rouge_scores['rouge-2']['f'],\n",
    "               rouge_scores['rouge-l']['r'], \n",
    "               rouge_scores['rouge-l']['p'], \n",
    "               rouge_scores['rouge-l']['f']\n",
    "               ]\n",
    "    \n",
    "    AllRouge_Scores.loc[-1] = ref_row\n",
    "    AllRouge_Scores.index = AllRouge_Scores.index + 1  #shift index\n",
    "    AllRouge_Scores = AllRouge_Scores.sort_index()  #sort by index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLM_Model</th>\n",
       "      <th>rouge-1-r</th>\n",
       "      <th>rouge-1-p</th>\n",
       "      <th>rouge-1-f</th>\n",
       "      <th>rouge-2-r</th>\n",
       "      <th>rouge-2-p</th>\n",
       "      <th>rouge-2-f</th>\n",
       "      <th>rouge-l-r</th>\n",
       "      <th>rouge-l-p</th>\n",
       "      <th>rouge-l-f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qwen2:latest</td>\n",
       "      <td>0.583885</td>\n",
       "      <td>0.380998</td>\n",
       "      <td>0.439601</td>\n",
       "      <td>0.293958</td>\n",
       "      <td>0.188656</td>\n",
       "      <td>0.217142</td>\n",
       "      <td>0.536320</td>\n",
       "      <td>0.349282</td>\n",
       "      <td>0.403292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3.1:latest</td>\n",
       "      <td>0.468117</td>\n",
       "      <td>0.544860</td>\n",
       "      <td>0.487968</td>\n",
       "      <td>0.253035</td>\n",
       "      <td>0.311468</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.430178</td>\n",
       "      <td>0.500524</td>\n",
       "      <td>0.448395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral:latest</td>\n",
       "      <td>0.567578</td>\n",
       "      <td>0.385404</td>\n",
       "      <td>0.444611</td>\n",
       "      <td>0.288155</td>\n",
       "      <td>0.192329</td>\n",
       "      <td>0.221757</td>\n",
       "      <td>0.525446</td>\n",
       "      <td>0.356232</td>\n",
       "      <td>0.411255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deepseek-llm:latest</td>\n",
       "      <td>0.398404</td>\n",
       "      <td>0.450444</td>\n",
       "      <td>0.401904</td>\n",
       "      <td>0.181900</td>\n",
       "      <td>0.212641</td>\n",
       "      <td>0.184795</td>\n",
       "      <td>0.361034</td>\n",
       "      <td>0.408113</td>\n",
       "      <td>0.363987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phi3:latest</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>0.090859</td>\n",
       "      <td>0.072026</td>\n",
       "      <td>0.010513</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>0.009198</td>\n",
       "      <td>0.066152</td>\n",
       "      <td>0.081534</td>\n",
       "      <td>0.063275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LLM_Model  rouge-1-r  rouge-1-p  rouge-1-f  rouge-2-r  rouge-2-p  \\\n",
       "0         qwen2:latest   0.583885   0.380998   0.439601   0.293958   0.188656   \n",
       "1      llama3.1:latest   0.468117   0.544860   0.487968   0.253035   0.311468   \n",
       "2       mistral:latest   0.567578   0.385404   0.444611   0.288155   0.192329   \n",
       "3  deepseek-llm:latest   0.398404   0.450444   0.401904   0.181900   0.212641   \n",
       "4          phi3:latest   0.075596   0.090859   0.072026   0.010513   0.009372   \n",
       "\n",
       "   rouge-2-f  rouge-l-r  rouge-l-p  rouge-l-f  \n",
       "0   0.217142   0.536320   0.349282   0.403292  \n",
       "1   0.269300   0.430178   0.500524   0.448395  \n",
       "2   0.221757   0.525446   0.356232   0.411255  \n",
       "3   0.184795   0.361034   0.408113   0.363987  \n",
       "4   0.009198   0.066152   0.081534   0.063275  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AllRouge_Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "import statistics\n",
    "import importlib\n",
    "importlib.reload(Eval_Metrics)\n",
    "bertscore = load(\"bertscore\")\n",
    "eval_metrics = Eval_Metrics.Evaluation_Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_3200_FINAL_backup.csv')\n",
    "Eval_df_copy =df.copy(deep=True)\n",
    "Eval_df_copy = Eval_df_copy.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi3:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_mean 0.7769536617337596\n",
      "precision_stdev 0.05196536175369054\n",
      "recall_mean 0.7884865583088865\n",
      "recall_stdev 0.051000813490244226\n",
      "f1_mean 0.7823995193490242\n",
      "f1_stdev 0.04928785532957433\n",
      "deepseek-llm:latest\n",
      "precision_mean 0.9065995134972036\n",
      "precision_stdev 0.029721384020192563\n",
      "recall_mean 0.8983758135698736\n",
      "recall_stdev 0.029521019105387965\n",
      "f1_mean 0.9021323166601359\n",
      "f1_stdev 0.024136782895646584\n",
      "mistral:latest\n",
      "precision_mean 0.8908155338466167\n",
      "precision_stdev 0.030417561403784703\n",
      "recall_mean 0.9232952133752406\n",
      "recall_stdev 0.02261668397260543\n",
      "f1_mean 0.9065113559551538\n",
      "f1_stdev 0.022490289846493698\n",
      "llama3.1:latest\n",
      "precision_mean 0.9262234579399228\n",
      "precision_stdev 0.02671300475743821\n",
      "recall_mean 0.9094036938063801\n",
      "recall_stdev 0.02875995661510874\n",
      "f1_mean 0.9174677292071283\n",
      "f1_stdev 0.023032587254509476\n",
      "qwen2:latest\n",
      "precision_mean 0.8904012389853597\n",
      "precision_stdev 0.03378242280398264\n",
      "recall_mean 0.9240397435612977\n",
      "recall_stdev 0.025273881654074626\n",
      "f1_mean 0.9065637845732272\n",
      "f1_stdev 0.024591971236326057\n"
     ]
    }
   ],
   "source": [
    "## BERTScore Scores\n",
    "AllBERT_Scores = pd.DataFrame()\n",
    "AllBERT_Scores = pd.DataFrame(columns=['LLM_Model','Precision Mean','Precision Stddev','Recall Mean','Recall Stddev','F1 Score Mean','F1 Score Stddev','hashcode'])\n",
    "\n",
    "# llm_model = ['phi3:latest','llama3.1:latest']\n",
    "for llm in llm_models: \n",
    "# for llm in llm_model: \n",
    "    print(llm)\n",
    "    GeneratedSummary_col = llm + \"_GenSummary\"\n",
    "    ModelSummary_col = 'model_summary'\n",
    "    \n",
    "    # results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    \n",
    "    test_bert_df = Eval_df_copy[[GeneratedSummary_col,ModelSummary_col]].copy(deep=True)\n",
    "    test_bert_df[GeneratedSummary_col] = test_bert_df[GeneratedSummary_col].replace('', np.nan)\n",
    "    test_bert_df = test_bert_df.dropna(subset=[GeneratedSummary_col])\n",
    "    test_bert_df = test_bert_df.reset_index(drop=True)\n",
    "    \n",
    "    BERT_Scores = bertscore.compute(predictions=test_bert_df[GeneratedSummary_col], references=test_bert_df[ModelSummary_col], lang=\"en\")\n",
    "    precision_mean = statistics.mean(BERT_Scores['precision'])\n",
    "    precision_stdev = statistics.stdev(BERT_Scores['precision'])\n",
    "    recall_mean = statistics.mean(BERT_Scores['recall'])\n",
    "    recall_stdev = statistics.stdev(BERT_Scores['recall'])\n",
    "    f1_mean = statistics.mean(BERT_Scores['f1'])\n",
    "    f1_stdev = statistics.stdev(BERT_Scores['f1'])\n",
    "    hash_code = BERT_Scores['hashcode']\n",
    "    \n",
    "    print('precision_mean',precision_mean)\n",
    "    print('precision_stdev',precision_stdev)\n",
    "    print('recall_mean',recall_mean)\n",
    "    print('recall_stdev',recall_stdev)\n",
    "    print('f1_mean',f1_mean)\n",
    "    print('f1_stdev',f1_stdev)\n",
    "    ref_row = [llm , \n",
    "               precision_mean, \n",
    "               precision_stdev, \n",
    "               recall_mean, \n",
    "               recall_stdev, \n",
    "               f1_mean, \n",
    "               f1_stdev,\n",
    "               hash_code\n",
    "               ]\n",
    "    \n",
    "    AllBERT_Scores.loc[-1] = ref_row\n",
    "    AllBERT_Scores.index = AllBERT_Scores.index + 1  #shift index\n",
    "    AllBERT_Scores = AllBERT_Scores.sort_index()  #sort by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLM_Model</th>\n",
       "      <th>Precision Mean</th>\n",
       "      <th>Precision Stddev</th>\n",
       "      <th>Recall Mean</th>\n",
       "      <th>Recall Stddev</th>\n",
       "      <th>F1 Score Mean</th>\n",
       "      <th>F1 Score Stddev</th>\n",
       "      <th>hashcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qwen2:latest</td>\n",
       "      <td>0.890401</td>\n",
       "      <td>0.033782</td>\n",
       "      <td>0.924040</td>\n",
       "      <td>0.025274</td>\n",
       "      <td>0.906564</td>\n",
       "      <td>0.024592</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3.1:latest</td>\n",
       "      <td>0.926223</td>\n",
       "      <td>0.026713</td>\n",
       "      <td>0.909404</td>\n",
       "      <td>0.028760</td>\n",
       "      <td>0.917468</td>\n",
       "      <td>0.023033</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral:latest</td>\n",
       "      <td>0.890816</td>\n",
       "      <td>0.030418</td>\n",
       "      <td>0.923295</td>\n",
       "      <td>0.022617</td>\n",
       "      <td>0.906511</td>\n",
       "      <td>0.022490</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deepseek-llm:latest</td>\n",
       "      <td>0.906600</td>\n",
       "      <td>0.029721</td>\n",
       "      <td>0.898376</td>\n",
       "      <td>0.029521</td>\n",
       "      <td>0.902132</td>\n",
       "      <td>0.024137</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phi3:latest</td>\n",
       "      <td>0.776954</td>\n",
       "      <td>0.051965</td>\n",
       "      <td>0.788487</td>\n",
       "      <td>0.051001</td>\n",
       "      <td>0.782400</td>\n",
       "      <td>0.049288</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LLM_Model  Precision Mean  Precision Stddev  Recall Mean  \\\n",
       "0         qwen2:latest        0.890401          0.033782     0.924040   \n",
       "1      llama3.1:latest        0.926223          0.026713     0.909404   \n",
       "2       mistral:latest        0.890816          0.030418     0.923295   \n",
       "3  deepseek-llm:latest        0.906600          0.029721     0.898376   \n",
       "4          phi3:latest        0.776954          0.051965     0.788487   \n",
       "\n",
       "   Recall Stddev  F1 Score Mean  F1 Score Stddev  \\\n",
       "0       0.025274       0.906564         0.024592   \n",
       "1       0.028760       0.917468         0.023033   \n",
       "2       0.022617       0.906511         0.022490   \n",
       "3       0.029521       0.902132         0.024137   \n",
       "4       0.051001       0.782400         0.049288   \n",
       "\n",
       "                                            hashcode  \n",
       "0  roberta-large_L17_no-idf_version=0.3.12(hug_tr...  \n",
       "1  roberta-large_L17_no-idf_version=0.3.12(hug_tr...  \n",
       "2  roberta-large_L17_no-idf_version=0.3.12(hug_tr...  \n",
       "3  roberta-large_L17_no-idf_version=0.3.12(hug_tr...  \n",
       "4  roberta-large_L17_no-idf_version=0.3.12(hug_tr...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AllBERT_Scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
