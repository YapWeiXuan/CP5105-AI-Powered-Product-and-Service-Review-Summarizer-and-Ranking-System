{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import requests\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "import ollama\n",
    "from evaluate import load\n",
    "from rouge import Rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import importlib\n",
    "import Eval_Metrics\n",
    "eval_metrics = Eval_Metrics.Evaluation_Metrics()\n",
    "# importlib.reload(Eval_Metrics)\n",
    "# eval_metrics = Eval_Metrics.Evaluation_Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.1:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Ranking List Prompt Dataset to be used in Ranking List LLM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_examples_df = pd.read_excel(r\"C:\\Users\\flame\\CP5105-AI-Powered-Product-and-Service-Review-Summarizer-and-Ranking-System\\BackEnd\\All Prompt Examples\\Ranking List Prompt Dataset.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_examples_df['Modified_Multiple_Generated_Summaries'] = prompt_examples_df['Modified_Multiple_Generated_Summaries'].map(lambda a: a.replace('\\n',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Query', 'Modified_1_Generated_Summary',\n",
       "       'Modified_Multiple_Generated_Summaries', 'Model_Ranking_List_Output',\n",
       "       'Model_Ranking_List_for_Manual_Checking', 'Prompt', 'Unnamed: 6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_examples_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Data to be used as Prompt Examples from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Modified_1_Generated_Summary</th>\n",
       "      <th>Modified_Multiple_Generated_Summaries</th>\n",
       "      <th>Model_Ranking_List_Output</th>\n",
       "      <th>Model_Ranking_List_for_Manual_Checking</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the worst mobile phone games released...</td>\n",
       "      <td>The gaming industry has been flooded with new ...</td>\n",
       "      <td>Article #1: The gaming industry has been flood...</td>\n",
       "      <td>Here is the ranking list for the worst mobile ...</td>\n",
       "      <td>1: Robot Rebellion\\n2. Galactic Conquest\\nRest...</td>\n",
       "      <td>Use for Prompt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the top 5 best sci-fi movies of all t...</td>\n",
       "      <td>This is a question that has been debated by fi...</td>\n",
       "      <td>Article #1:  This is a question that has been ...</td>\n",
       "      <td>Here is the ranking list for the top 5 best sc...</td>\n",
       "      <td>1: Blade Runner\\n2: The Matrix\\n3: A Space Ody...</td>\n",
       "      <td>Use for Prompt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the 3 worst games consoles of all time?</td>\n",
       "      <td>The Atari Jaguar is often cited as one of the ...</td>\n",
       "      <td>Article #1:  The Atari Jaguar is often cited a...</td>\n",
       "      <td>Here is the ranking list for the 3 worst games...</td>\n",
       "      <td>1,2: Atari Jaguar, Nintendo Virtual Boy\\n3: Se...</td>\n",
       "      <td>Use for Prompt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the 5 worst board games?</td>\n",
       "      <td>Determining the worst board games can be subje...</td>\n",
       "      <td>Article #1: Determining the worst board games ...</td>\n",
       "      <td>Here is the ranking list for the 5 worst board...</td>\n",
       "      <td>1: Monikers\\n2: Don’t Wake Daddy\\n3: Battleshi...</td>\n",
       "      <td>Use for Prompt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the best printer models in 2024?</td>\n",
       "      <td>Choosing the right printer can be overwhelming...</td>\n",
       "      <td>Article #1: Choosing the right printer can be ...</td>\n",
       "      <td>Here is the ranking list for the best printer ...</td>\n",
       "      <td>1. Epson Expression Photo HD XP-15000\\n2. Xero...</td>\n",
       "      <td>Use for Prompt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0  What are the worst mobile phone games released...   \n",
       "1  What are the top 5 best sci-fi movies of all t...   \n",
       "2   What are the 3 worst games consoles of all time?   \n",
       "3                  What are the 5 worst board games?   \n",
       "4          What are the best printer models in 2024?   \n",
       "\n",
       "                        Modified_1_Generated_Summary  \\\n",
       "0  The gaming industry has been flooded with new ...   \n",
       "1  This is a question that has been debated by fi...   \n",
       "2  The Atari Jaguar is often cited as one of the ...   \n",
       "3  Determining the worst board games can be subje...   \n",
       "4  Choosing the right printer can be overwhelming...   \n",
       "\n",
       "               Modified_Multiple_Generated_Summaries  \\\n",
       "0  Article #1: The gaming industry has been flood...   \n",
       "1  Article #1:  This is a question that has been ...   \n",
       "2  Article #1:  The Atari Jaguar is often cited a...   \n",
       "3  Article #1: Determining the worst board games ...   \n",
       "4  Article #1: Choosing the right printer can be ...   \n",
       "\n",
       "                           Model_Ranking_List_Output  \\\n",
       "0  Here is the ranking list for the worst mobile ...   \n",
       "1  Here is the ranking list for the top 5 best sc...   \n",
       "2  Here is the ranking list for the 3 worst games...   \n",
       "3  Here is the ranking list for the 5 worst board...   \n",
       "4  Here is the ranking list for the best printer ...   \n",
       "\n",
       "              Model_Ranking_List_for_Manual_Checking          Prompt  \\\n",
       "0  1: Robot Rebellion\\n2. Galactic Conquest\\nRest...  Use for Prompt   \n",
       "1  1: Blade Runner\\n2: The Matrix\\n3: A Space Ody...  Use for Prompt   \n",
       "2  1,2: Atari Jaguar, Nintendo Virtual Boy\\n3: Se...  Use for Prompt   \n",
       "3  1: Monikers\\n2: Don’t Wake Daddy\\n3: Battleshi...  Use for Prompt   \n",
       "4  1. Epson Expression Photo HD XP-15000\\n2. Xero...  Use for Prompt   \n",
       "\n",
       "  Unnamed: 6  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptengin_df = prompt_examples_df[prompt_examples_df['Prompt']=='Use for Prompt'].reset_index(drop=True)\n",
    "promptengin_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Data to be used for Ranking List LLM testing from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Modified_1_Generated_Summary</th>\n",
       "      <th>Modified_Multiple_Generated_Summaries</th>\n",
       "      <th>Model_Ranking_List_Output</th>\n",
       "      <th>Model_Ranking_List_for_Manual_Checking</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the top car models in 2024?</td>\n",
       "      <td>The automotive industry is constantly evolving...</td>\n",
       "      <td>Article #1: The automotive industry is constan...</td>\n",
       "      <td>Here is the ranking list for the top car model...</td>\n",
       "      <td>1,2: Toyota Camry , Honda Civic Type R\\n3,4: H...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are the top car models in 2024?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the 3 worst laptop brands in 2024?</td>\n",
       "      <td>The ranking of laptop brands can vary dependin...</td>\n",
       "      <td>Article #1: The ranking of laptop brands can v...</td>\n",
       "      <td>Here is the ranking list for the 3 worst lapto...</td>\n",
       "      <td>1: Acer\\n2: Lenovo\\n3: Asus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are the 3 worst laptop brands in 2024?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the cheapest but decent laptop models...</td>\n",
       "      <td>When it comes to budget-friendly laptops, ther...</td>\n",
       "      <td>Article #1: When it comes to budget-friendly l...</td>\n",
       "      <td>Here is the ranking list for the top budget la...</td>\n",
       "      <td>1: Lenovo IdeaPad 330S\\n2,3, 4 ,5: Acer Aspire...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are the cheapest but decent laptop models...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the top sunscreen brands?</td>\n",
       "      <td>Sunscreen is a vital part of our daily skincar...</td>\n",
       "      <td>Article #1:  Sunscreen is a vital part of our ...</td>\n",
       "      <td>Here is the ranking list for the top sunscreen...</td>\n",
       "      <td>1. Neutrogena\\n2. Hawaiian Tropic\\n3,4,5: La R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are the top sunscreen brands?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the most interesting and top rated bo...</td>\n",
       "      <td>The year 2024 is expected to bring a diverse r...</td>\n",
       "      <td>Article #1: The year 2024 is expected to bring...</td>\n",
       "      <td>Here is the ranking list for the most interest...</td>\n",
       "      <td>1,2: The It Girl, The Memory Keeper\\'s Daughte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are the most interesting and top rated bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the most hated and worst books releas...</td>\n",
       "      <td>Based on user ratings on Goodreads, some of th...</td>\n",
       "      <td>Article #1: Based on user ratings on Goodreads...</td>\n",
       "      <td>Here is the ranking list for the most hated an...</td>\n",
       "      <td>1,2: No Going Back , Crown of Starlight\\n3,4 :...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are the most hated and worst books releas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the 5 worst horror movies of all time?</td>\n",
       "      <td>The assessment of the worst horror movies is s...</td>\n",
       "      <td>Article #1: The assessment of the worst horror...</td>\n",
       "      <td>Here is the ranking list for the 5 worst horro...</td>\n",
       "      <td>1: Devil\\n2: Birdemic: Shock and Terror (2010)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are the 5 worst horror movies of all time?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are the 3 best games consoles of all time?</td>\n",
       "      <td>Determining the \"best\" gaming consoles is subj...</td>\n",
       "      <td>Article #1: Determining the \"best\" gaming cons...</td>\n",
       "      <td>Here is the ranking list for the top 3 best ga...</td>\n",
       "      <td>1,2: PlayStation 2 (PS2) , Nintendo Entertainm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are the 3 best games consoles of all time?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the 5 best board games for children?</td>\n",
       "      <td>Choosing the right board game can be a challen...</td>\n",
       "      <td>Article #1: Choosing the right board game can ...</td>\n",
       "      <td>Here is the ranking list for the 5 best board ...</td>\n",
       "      <td>1,2,3,4 : Candy Land, Chutes and Ladders, Mono...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are the 5 best board games for children?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the worst mobile phones in 2024?</td>\n",
       "      <td>The ranking of the worst mobile phones can be ...</td>\n",
       "      <td>Article #1: The ranking of the worst mobile ph...</td>\n",
       "      <td>Here is the ranking list for the worst mobile ...</td>\n",
       "      <td>1. Motorola Defy Pro X\\n2,3: HTC One M7, Samsu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are the worst mobile phones in 2024?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0               What are the top car models in 2024?   \n",
       "1        What are the 3 worst laptop brands in 2024?   \n",
       "2  What are the cheapest but decent laptop models...   \n",
       "3                 What are the top sunscreen brands?   \n",
       "4  What are the most interesting and top rated bo...   \n",
       "5  What are the most hated and worst books releas...   \n",
       "6    What are the 5 worst horror movies of all time?   \n",
       "7    What are the 3 best games consoles of all time?   \n",
       "8      What are the 5 best board games for children?   \n",
       "9          What are the worst mobile phones in 2024?   \n",
       "\n",
       "                        Modified_1_Generated_Summary  \\\n",
       "0  The automotive industry is constantly evolving...   \n",
       "1  The ranking of laptop brands can vary dependin...   \n",
       "2  When it comes to budget-friendly laptops, ther...   \n",
       "3  Sunscreen is a vital part of our daily skincar...   \n",
       "4  The year 2024 is expected to bring a diverse r...   \n",
       "5  Based on user ratings on Goodreads, some of th...   \n",
       "6  The assessment of the worst horror movies is s...   \n",
       "7  Determining the \"best\" gaming consoles is subj...   \n",
       "8  Choosing the right board game can be a challen...   \n",
       "9  The ranking of the worst mobile phones can be ...   \n",
       "\n",
       "               Modified_Multiple_Generated_Summaries  \\\n",
       "0  Article #1: The automotive industry is constan...   \n",
       "1  Article #1: The ranking of laptop brands can v...   \n",
       "2  Article #1: When it comes to budget-friendly l...   \n",
       "3  Article #1:  Sunscreen is a vital part of our ...   \n",
       "4  Article #1: The year 2024 is expected to bring...   \n",
       "5  Article #1: Based on user ratings on Goodreads...   \n",
       "6  Article #1: The assessment of the worst horror...   \n",
       "7  Article #1: Determining the \"best\" gaming cons...   \n",
       "8  Article #1: Choosing the right board game can ...   \n",
       "9  Article #1: The ranking of the worst mobile ph...   \n",
       "\n",
       "                           Model_Ranking_List_Output  \\\n",
       "0  Here is the ranking list for the top car model...   \n",
       "1  Here is the ranking list for the 3 worst lapto...   \n",
       "2  Here is the ranking list for the top budget la...   \n",
       "3  Here is the ranking list for the top sunscreen...   \n",
       "4  Here is the ranking list for the most interest...   \n",
       "5  Here is the ranking list for the most hated an...   \n",
       "6  Here is the ranking list for the 5 worst horro...   \n",
       "7  Here is the ranking list for the top 3 best ga...   \n",
       "8  Here is the ranking list for the 5 best board ...   \n",
       "9  Here is the ranking list for the worst mobile ...   \n",
       "\n",
       "              Model_Ranking_List_for_Manual_Checking Prompt  \\\n",
       "0  1,2: Toyota Camry , Honda Civic Type R\\n3,4: H...    NaN   \n",
       "1                        1: Acer\\n2: Lenovo\\n3: Asus    NaN   \n",
       "2  1: Lenovo IdeaPad 330S\\n2,3, 4 ,5: Acer Aspire...    NaN   \n",
       "3  1. Neutrogena\\n2. Hawaiian Tropic\\n3,4,5: La R...    NaN   \n",
       "4  1,2: The It Girl, The Memory Keeper\\'s Daughte...    NaN   \n",
       "5  1,2: No Going Back , Crown of Starlight\\n3,4 :...    NaN   \n",
       "6  1: Devil\\n2: Birdemic: Shock and Terror (2010)...    NaN   \n",
       "7  1,2: PlayStation 2 (PS2) , Nintendo Entertainm...    NaN   \n",
       "8  1,2,3,4 : Candy Land, Chutes and Ladders, Mono...    NaN   \n",
       "9  1. Motorola Defy Pro X\\n2,3: HTC One M7, Samsu...    NaN   \n",
       "\n",
       "                                          Unnamed: 6  \n",
       "0               What are the top car models in 2024?  \n",
       "1        What are the 3 worst laptop brands in 2024?  \n",
       "2  What are the cheapest but decent laptop models...  \n",
       "3                 What are the top sunscreen brands?  \n",
       "4  What are the most interesting and top rated bo...  \n",
       "5  What are the most hated and worst books releas...  \n",
       "6    What are the 5 worst horror movies of all time?  \n",
       "7    What are the 3 best games consoles of all time?  \n",
       "8      What are the 5 best board games for children?  \n",
       "9          What are the worst mobile phones in 2024?  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testprompt_df = prompt_examples_df[prompt_examples_df['Prompt'].isna()==True].reset_index(drop=True)\n",
    "testprompt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking List response generation function for Zero Step COT SCT text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZERO_STEP_SYSTEM_COT_SINGLE_generate_report_with_query(text, model,query,prompt):\n",
    "               \n",
    "\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': prompt + f'''\n",
    "            Let\\'s think step by step.\n",
    "            1) Identify the item names relevant to the {query} in the user prompt text.\n",
    "            2) Calculate number of times each item name is mentioned in the user prompt text.\n",
    "            3) Create an ordered ranking list ordered from item name that appeared most amount of times in the user prompt text to the item name that appeared least amount of times in the user prompt text.\n",
    "            4) Provide a description for each item name in the ordered ranking list.\n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'''In the context of \"{query}\" , generate an ordered and ranked list from the following text : '''\n",
    "            + text,\n",
    "        },\n",
    "\n",
    "        ]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    report = response['message']['content']\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking List response generation function for Zero Step SCT text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SINGLE_generate_report_with_query(text, model,query,prompt):\n",
    "               \n",
    "\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'In the context of \"{query}\" , generate an ordered and ranked list from the following text : ' \n",
    "            + text,\n",
    "        },\n",
    "\n",
    "        ]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    report = response['message']['content']\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking List response generation function for Zero Shot (0.96 Temp , 0.9 Top P , 40 Top K ) SYS with SCT text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SINGLE_0_96T_generate_report_with_query(text, model,query,prompt):\n",
    "               \n",
    "\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'In the context of \"{query}\" , generate an ordered and ranked list from the following text : ' \n",
    "            + text,\n",
    "        },\n",
    "        ],\n",
    "        options = {\n",
    "            \"temperature\": 0.96\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    report = response['message']['content']\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking List response generation function for Zero Shot (0.64 Temp , 0.9 Top P , 40 Top K ) SYS with SCT text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SINGLE_0_64T_generate_report_with_query(text, model,query,prompt):\n",
    "               \n",
    "\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'In the context of \"{query}\" , generate an ordered and ranked list from the following text : ' \n",
    "            + text,\n",
    "        },\n",
    "        ],\n",
    "        options = {\n",
    "            \"temperature\": 0.64\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    report = response['message']['content']\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking List response generation function for Zero Shot (0.8 Temp , 0.9 Top P , 48 Top K ) SYS with SCT text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SINGLE_48topk_generate_report_with_query(text, model,query,prompt):\n",
    "               \n",
    "\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'In the context of \"{query}\" , generate an ordered and ranked list from the following text : ' \n",
    "            + text,\n",
    "        },\n",
    "        ],\n",
    "        options = {\n",
    "            \"top_k\": 48\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    report = response['message']['content']\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking List response generation function for Zero Shot (0.8 Temp , 0.9 Top P , 32 Top K ) SYS with SCT text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SINGLE_32topk_generate_report_with_query(text, model,query,prompt):\n",
    "               \n",
    "\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'In the context of \"{query}\" , generate an ordered and ranked list from the following text : ' \n",
    "            + text,\n",
    "        },\n",
    "        ],\n",
    "        options = {\n",
    "            \"top_k\": 32\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    report = response['message']['content']\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking List response generation function for Zero Shot (0.8 Temp , 1.08 Top P , 40 Top K ) SYS with SCT text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SINGLE_1_08topp_generate_report_with_query(text, model,query,prompt):\n",
    "               \n",
    "\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'In the context of \"{query}\" , generate an ordered and ranked list from the following text : ' \n",
    "            + text,\n",
    "        },\n",
    "        ],\n",
    "        options = {\n",
    "            \"top_p\": 1.08\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    report = response['message']['content']\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking List response generation function for Zero Shot (0.8 Temp , 0.72 Top P , 48 Top K ) SYS with SCT text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SINGLE_0_72topp_generate_report_with_query(text, model,query,prompt):\n",
    "               \n",
    "\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'In the context of \"{query}\" , generate an ordered and ranked list from the following text : ' \n",
    "            + text,\n",
    "        },\n",
    "        ],\n",
    "        options = {\n",
    "            \"top_p\": 0.72\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    report = response['message']['content']\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataframe Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_response_df = pd.DataFrame(columns=['query','model_response','zero_shot_response_1','zero_shot_response_1_score','zero_shot_response_2','zero_shot_response_2_score','avg_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chosen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.1:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt_SINGLE(query):\n",
    "        zero_shot_prompt =  f'''You are a reviewer generating an ordered and numbered ranked list based on {query} only. \\\n",
    "                Each item in the ordered list should be ordered from item that appeared most amount of times in the user prompt text to the item that appeared least amount of times in the user prompt text. \\\n",
    "                Each item in the ordered list should have the item name and the item details from the user prompt text. \\\n",
    "                Bold each item on the ranking list in the response.\\\n",
    "                To make text bold, enclose it with double asterisks (**). Example: **text** becomes bold.\\\n",
    "                '''\n",
    "        return zero_shot_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_single_response_df = pd.DataFrame(columns=['query','model_response','zero_shot_response_1','zero_shot_response_1_score','zero_shot_response_2','zero_shot_response_2_score','zero_shot_response_3','zero_shot_response_3_score','avg_score'])\n",
    "\n",
    "for index ,row in testprompt_df.iterrows():\n",
    "    text = row.Modified_1_Generated_Summary\n",
    "    query = row.Query\n",
    "    prompt = zero_shot_prompt_SINGLE(query)\n",
    "    model_response = row.Model_Ranking_List_Output\n",
    "    zero_shot_response_1_score = ''\n",
    "    zero_shot_response_2_score = ''\n",
    "    zero_shot_response_3_score = ''\n",
    "    avg_score = ''\n",
    "    \n",
    "    response_text_1 = SINGLE_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_2 = SINGLE_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_3 = SINGLE_generate_report_with_query(text, model,query,prompt)\n",
    "\n",
    "    ref_row = [query , \n",
    "               model_response, \n",
    "               response_text_1, \n",
    "               zero_shot_response_1_score, \n",
    "               response_text_2, \n",
    "               zero_shot_response_2_score, \n",
    "               response_text_3,\n",
    "               zero_shot_response_3_score,\n",
    "               \n",
    "               avg_score,\n",
    "               ]\n",
    "    \n",
    "    zero_shot_single_response_df.loc[-1] = ref_row\n",
    "    zero_shot_single_response_df.index = zero_shot_single_response_df.index + 1  #shift index\n",
    "    zero_shot_single_response_df = zero_shot_single_response_df.sort_index()  #sort by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_single_response_df.to_csv('FINETUNE_zero_shot_single_response_df_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Temp = 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_0_96T_zero_shot_single_response_df = pd.DataFrame(columns=['query','model_response','zero_shot_response_1','zero_shot_response_1_score','zero_shot_response_2','zero_shot_response_2_score','zero_shot_response_3','zero_shot_response_3_score','avg_score'])\n",
    "\n",
    "for index ,row in testprompt_df.iterrows():\n",
    "    text = row.Modified_1_Generated_Summary\n",
    "    query = row.Query\n",
    "    prompt = zero_shot_prompt_SINGLE(query)\n",
    "    model_response = row.Model_Ranking_List_Output\n",
    "    zero_shot_response_1_score = ''\n",
    "    zero_shot_response_2_score = ''\n",
    "    zero_shot_response_3_score = ''\n",
    "    avg_score = ''\n",
    "    \n",
    "    response_text_1 = SINGLE_0_96T_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_2 = SINGLE_0_96T_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_3 = SINGLE_0_96T_generate_report_with_query(text, model,query,prompt)\n",
    "\n",
    "    ref_row = [query , \n",
    "               model_response, \n",
    "               response_text_1, \n",
    "               zero_shot_response_1_score, \n",
    "               response_text_2, \n",
    "               zero_shot_response_2_score, \n",
    "                response_text_3,\n",
    "               zero_shot_response_3_score,\n",
    "               avg_score,\n",
    "               ]\n",
    "    \n",
    "    SINGLE_0_96T_zero_shot_single_response_df.loc[-1] = ref_row\n",
    "    SINGLE_0_96T_zero_shot_single_response_df.index = SINGLE_0_96T_zero_shot_single_response_df.index + 1  #shift index\n",
    "    SINGLE_0_96T_zero_shot_single_response_df = SINGLE_0_96T_zero_shot_single_response_df.sort_index()  #sort by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_0_96T_zero_shot_single_response_df.to_csv('FINETUNE_SINGLE_0_96T_zero_shot_single_response_df_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Temp = 0.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_0_64T_zero_shot_single_response_df = pd.DataFrame(columns=['query','model_response','zero_shot_response_1','zero_shot_response_1_score','zero_shot_response_2','zero_shot_response_2_score','zero_shot_response_3','zero_shot_response_3_score','avg_score'])\n",
    "\n",
    "for index ,row in testprompt_df.iterrows():\n",
    "    text = row.Modified_1_Generated_Summary\n",
    "    query = row.Query\n",
    "    prompt = zero_shot_prompt_SINGLE(query)\n",
    "    model_response = row.Model_Ranking_List_Output\n",
    "    zero_shot_response_1_score = ''\n",
    "    zero_shot_response_2_score = ''\n",
    "    zero_shot_response_3_score = ''\n",
    "    avg_score = ''\n",
    "    \n",
    "    response_text_1 = SINGLE_0_64T_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_2 = SINGLE_0_64T_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_3 = SINGLE_0_64T_generate_report_with_query(text, model,query,prompt)\n",
    "\n",
    "    ref_row = [query , \n",
    "               model_response, \n",
    "               response_text_1, \n",
    "               zero_shot_response_1_score, \n",
    "               response_text_2, \n",
    "               zero_shot_response_2_score, \n",
    "               response_text_3,\n",
    "               zero_shot_response_3_score,\n",
    "               avg_score,\n",
    "               ]\n",
    "    \n",
    "    SINGLE_0_64T_zero_shot_single_response_df.loc[-1] = ref_row\n",
    "    SINGLE_0_64T_zero_shot_single_response_df.index = SINGLE_0_64T_zero_shot_single_response_df.index + 1  #shift index\n",
    "    SINGLE_0_64T_zero_shot_single_response_df = SINGLE_0_64T_zero_shot_single_response_df.sort_index()  #sort by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_0_64T_zero_shot_single_response_df.to_csv('FINETUNE_SINGLE_0_64T_zero_shot_single_response_df_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Top K = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_48topk_zero_shot_single_response_df = pd.DataFrame(columns=['query','model_response','zero_shot_response_1','zero_shot_response_1_score','zero_shot_response_2','zero_shot_response_2_score','zero_shot_response_3','zero_shot_response_3_score','avg_score'])\n",
    "\n",
    "for index ,row in testprompt_df.iterrows():\n",
    "    text = row.Modified_1_Generated_Summary\n",
    "    query = row.Query\n",
    "    prompt = zero_shot_prompt_SINGLE(query)\n",
    "    model_response = row.Model_Ranking_List_Output\n",
    "    zero_shot_response_1_score = ''\n",
    "    zero_shot_response_2_score = ''\n",
    "    zero_shot_response_3_score = ''\n",
    "    avg_score = ''\n",
    "    \n",
    "    response_text_1 = SINGLE_48topk_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_2 = SINGLE_48topk_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_3 = SINGLE_48topk_generate_report_with_query(text, model,query,prompt)\n",
    "\n",
    "    ref_row = [query , \n",
    "               model_response, \n",
    "               response_text_1, \n",
    "               zero_shot_response_1_score, \n",
    "               response_text_2, \n",
    "               zero_shot_response_2_score, \n",
    "               response_text_3,\n",
    "               zero_shot_response_3_score,\n",
    "               avg_score,\n",
    "               ]\n",
    "    \n",
    "    SINGLE_48topk_zero_shot_single_response_df.loc[-1] = ref_row\n",
    "    SINGLE_48topk_zero_shot_single_response_df.index = SINGLE_48topk_zero_shot_single_response_df.index + 1  #shift index\n",
    "    SINGLE_48topk_zero_shot_single_response_df = SINGLE_48topk_zero_shot_single_response_df.sort_index()  #sort by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_48topk_zero_shot_single_response_df.to_csv('FINETUNE_SINGLE_48topk_zero_shot_single_response_df_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Top K = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_32topk_zero_shot_single_response_df = pd.DataFrame(columns=['query','model_response','zero_shot_response_1','zero_shot_response_1_score','zero_shot_response_2','zero_shot_response_2_score','zero_shot_response_3','zero_shot_response_3_score','avg_score'])\n",
    "\n",
    "for index ,row in testprompt_df.iterrows():\n",
    "    text = row.Modified_1_Generated_Summary\n",
    "    query = row.Query\n",
    "    prompt = zero_shot_prompt_SINGLE(query)\n",
    "    model_response = row.Model_Ranking_List_Output\n",
    "    zero_shot_response_1_score = ''\n",
    "    zero_shot_response_2_score = ''\n",
    "    zero_shot_response_3_score = ''\n",
    "    avg_score = ''\n",
    "    \n",
    "    response_text_1 = SINGLE_32topk_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_2 = SINGLE_32topk_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_3 = SINGLE_32topk_generate_report_with_query(text, model,query,prompt)\n",
    "\n",
    "    ref_row = [query , \n",
    "               model_response, \n",
    "               response_text_1, \n",
    "               zero_shot_response_1_score, \n",
    "               response_text_2, \n",
    "               zero_shot_response_2_score, \n",
    "                response_text_3,\n",
    "               zero_shot_response_3_score,\n",
    "               avg_score,\n",
    "               ]\n",
    "    \n",
    "    SINGLE_32topk_zero_shot_single_response_df.loc[-1] = ref_row\n",
    "    SINGLE_32topk_zero_shot_single_response_df.index = SINGLE_32topk_zero_shot_single_response_df.index + 1  #shift index\n",
    "    SINGLE_32topk_zero_shot_single_response_df = SINGLE_32topk_zero_shot_single_response_df.sort_index()  #sort by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_32topk_zero_shot_single_response_df.to_csv('FINETUNE_SINGLE_32topk_zero_shot_single_response_df_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Top P = 1.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_1_08topp_zero_shot_single_response_df = pd.DataFrame(columns=['query','model_response','zero_shot_response_1','zero_shot_response_1_score','zero_shot_response_2','zero_shot_response_2_score','zero_shot_response_3','zero_shot_response_3_score','avg_score'])\n",
    "\n",
    "for index ,row in testprompt_df.iterrows():\n",
    "    text = row.Modified_1_Generated_Summary\n",
    "    query = row.Query\n",
    "    prompt = zero_shot_prompt_SINGLE(query)\n",
    "    model_response = row.Model_Ranking_List_Output\n",
    "    zero_shot_response_1_score = ''\n",
    "    zero_shot_response_2_score = ''\n",
    "    zero_shot_response_3_score = ''\n",
    "    avg_score = ''\n",
    "    \n",
    "    response_text_1 = SINGLE_1_08topp_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_2 = SINGLE_1_08topp_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_3 = SINGLE_1_08topp_generate_report_with_query(text, model,query,prompt)\n",
    "\n",
    "    ref_row = [query , \n",
    "               model_response, \n",
    "               response_text_1, \n",
    "               zero_shot_response_1_score, \n",
    "               response_text_2, \n",
    "               zero_shot_response_2_score, \n",
    "                response_text_3,\n",
    "               zero_shot_response_3_score,\n",
    "               avg_score,\n",
    "               ]\n",
    "    \n",
    "    SINGLE_1_08topp_zero_shot_single_response_df.loc[-1] = ref_row\n",
    "    SINGLE_1_08topp_zero_shot_single_response_df.index = SINGLE_1_08topp_zero_shot_single_response_df.index + 1  #shift index\n",
    "    SINGLE_1_08topp_zero_shot_single_response_df = SINGLE_1_08topp_zero_shot_single_response_df.sort_index()  #sort by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_1_08topp_zero_shot_single_response_df.to_csv('FINETUNE_SINGLE_1_08topp_zero_shot_single_response_df_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Top P = 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_0_72topp_zero_shot_single_response_df = pd.DataFrame(columns=['query','model_response','zero_shot_response_1','zero_shot_response_1_score','zero_shot_response_2','zero_shot_response_2_score','zero_shot_response_3','zero_shot_response_3_score','avg_score'])\n",
    "\n",
    "for index ,row in testprompt_df.iterrows():\n",
    "    text = row.Modified_1_Generated_Summary\n",
    "    query = row.Query\n",
    "    prompt = zero_shot_prompt_SINGLE(query)\n",
    "    model_response = row.Model_Ranking_List_Output\n",
    "    zero_shot_response_1_score = ''\n",
    "    zero_shot_response_2_score = ''\n",
    "    zero_shot_response_3_score = ''\n",
    "    avg_score = ''\n",
    "    \n",
    "    response_text_1 = SINGLE_0_72topp_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_2 = SINGLE_0_72topp_generate_report_with_query(text, model,query,prompt)\n",
    "    response_text_3 = SINGLE_0_72topp_generate_report_with_query(text, model,query,prompt)\n",
    "\n",
    "    ref_row = [query , \n",
    "               model_response, \n",
    "               response_text_1, \n",
    "               zero_shot_response_1_score, \n",
    "               response_text_2, \n",
    "               zero_shot_response_2_score, \n",
    "               response_text_3,\n",
    "               zero_shot_response_3_score,\n",
    "               avg_score,\n",
    "               ]\n",
    "    \n",
    "    SINGLE_0_72topp_zero_shot_single_response_df.loc[-1] = ref_row\n",
    "    SINGLE_0_72topp_zero_shot_single_response_df.index = SINGLE_0_72topp_zero_shot_single_response_df.index + 1  #shift index\n",
    "    SINGLE_0_72topp_zero_shot_single_response_df = SINGLE_0_72topp_zero_shot_single_response_df.sort_index()  #sort by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_0_72topp_zero_shot_single_response_df.to_csv('FINETUNE_SINGLE_0_72topp_zero_shot_single_response_df_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting BERTScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response_cols :  ['zero_shot_response_1', 'zero_shot_response_2']\n",
      "technique_name :  zero_STEP_WITH_EX_COT_SYS_single_response_df\n",
      "BERT_Scores_1 :  {'precision': [0.8748749494552612, 0.8916327357292175, 0.9043223857879639, 0.8711660504341125, 0.8965637683868408, 0.8562110662460327, 0.8699185252189636, 0.8661855459213257, 0.9063277840614319, 0.8756944537162781], 'recall': [0.8488946557044983, 0.8533327579498291, 0.8681201934814453, 0.8589116334915161, 0.8532718420028687, 0.8507874011993408, 0.8593361377716064, 0.8591965436935425, 0.8580129742622375, 0.8469337224960327], 'f1': [0.8616889715194702, 0.8720623850822449, 0.8858515620231628, 0.8649954199790955, 0.8743822574615479, 0.8534905910491943, 0.8645949363708496, 0.8626769185066223, 0.8815088272094727, 0.8610740303993225], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.48.2)'}\n",
      "BERT_Scores_2 :  {'precision': [0.875113308429718, 0.9153162837028503, 0.8849759101867676, 0.8580702543258667, 0.874768078327179, 0.8383445143699646, 0.8982372879981995, 0.8544772863388062, 0.914015531539917, 0.8876309990882874], 'recall': [0.8576751351356506, 0.8852801322937012, 0.8505675196647644, 0.8427568674087524, 0.8464216589927673, 0.7723644971847534, 0.8598189353942871, 0.8571126461029053, 0.8774328231811523, 0.8542784452438354], 'f1': [0.866306483745575, 0.9000476598739624, 0.867430567741394, 0.8503445982933044, 0.8603614568710327, 0.8040031790733337, 0.8786083459854126, 0.8557929396629333, 0.8953506350517273, 0.8706353902816772], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.48.2)'}\n",
      "ref_row :  ['zero_STEP_WITH_EX_COT_SYS_single_response_df', 0.8806923359632493, 0.02121979779950238, 0.8530253261327743, 0.01833242880032577, 0.8665603578090668, 0.018363844357678523, 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.48.2)']\n",
      "Response_cols :  ['zero_shot_response_1', 'zero_shot_response_2']\n",
      "technique_name :  zero_shot_NO_STEPS_COT_SYS_single_response_df\n",
      "BERT_Scores_1 :  {'precision': [0.892072319984436, 0.9227606654167175, 0.9001781344413757, 0.910861611366272, 0.8999438285827637, 0.8559510111808777, 0.8957118988037109, 0.8630733489990234, 0.8978742361068726, 0.8954047560691833], 'recall': [0.8828482031822205, 0.9017627835273743, 0.8633900284767151, 0.8866649270057678, 0.8578510880470276, 0.846998393535614, 0.8677399158477783, 0.8586637377738953, 0.8735620379447937, 0.882453441619873], 'f1': [0.8874363303184509, 0.9121409058570862, 0.8814003467559814, 0.8986003994941711, 0.8783934712409973, 0.8514511585235596, 0.8815040588378906, 0.8608628511428833, 0.8855513334274292, 0.8888819217681885], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.48.2)'}\n",
      "BERT_Scores_2 :  {'precision': [0.8923528790473938, 0.9069089889526367, 0.91767418384552, 0.9015856385231018, 0.8863586187362671, 0.8329416513442993, 0.8824582695960999, 0.8651805520057678, 0.9013354182243347, 0.87333744764328], 'recall': [0.8773555159568787, 0.8885831832885742, 0.8674750328063965, 0.8741359710693359, 0.8336377739906311, 0.8240271806716919, 0.8747607469558716, 0.8499614000320435, 0.8735592365264893, 0.8730533719062805], 'f1': [0.8847906589508057, 0.8976525664329529, 0.8918687701225281, 0.8876486420631409, 0.8591901659965515, 0.8284604549407959, 0.8785926103591919, 0.8575034737586975, 0.8872299790382385, 0.8731953501701355], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.48.2)'}\n",
      "ref_row :  ['zero_shot_NO_STEPS_COT_SYS_single_response_df', 0.8896982729434967, 0.022237841985011157, 0.8679241985082626, 0.018619226807306363, 0.8786177724599837, 0.01906252418414474, 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.48.2)']\n"
     ]
    }
   ],
   "source": [
    "## BERT Scores\n",
    "AllBERT_Scores = pd.DataFrame()\n",
    "AllBERT_Scores = pd.DataFrame(columns=['Prompt Engineering Technique','Precision Mean','Precision Stddev','Recall Mean','Recall Stddev','F1 Score Mean','F1 Score Stddev','hashcode'])\n",
    "\n",
    "# llm_model = ['phi3:latest','llama3.1:latest']\n",
    "for response_df_index in range(len(response_df_list)): \n",
    "    Response_cols = list(filter(lambda x: (('score' not in x) and ('shot_response' in x)) == True, list(response_df_list[response_df_index].columns)))\n",
    "    print('Response_cols : ',Response_cols)\n",
    "    ModelSummary_col = 'model_response'\n",
    "    technique_name = response_df_list_names[response_df_index]\n",
    "    print('technique_name : ',technique_name)\n",
    "    \n",
    "    \n",
    "    response_df_counter=1\n",
    "    for Response_col in Response_cols:\n",
    "        \n",
    "        if response_df_counter ==1:\n",
    "            test_bert_df = response_df_list[response_df_index][[Response_col,ModelSummary_col]].copy(deep=True)\n",
    "            test_bert_df[Response_col] = test_bert_df[Response_col].replace('', np.nan)\n",
    "            test_bert_df = test_bert_df.dropna(subset=[Response_col])\n",
    "            test_bert_df = test_bert_df.reset_index(drop=True)\n",
    "            \n",
    "            BERT_Scores_1 = bertscore.compute(predictions=test_bert_df[Response_col], references=test_bert_df[ModelSummary_col], lang=\"en\")\n",
    "            precision_mean_1 = statistics.mean(BERT_Scores_1['precision'])\n",
    "            precision_stdev_1 = statistics.stdev(BERT_Scores_1['precision'])\n",
    "            recall_mean_1 = statistics.mean(BERT_Scores_1['recall'])\n",
    "            recall_stdev_1 = statistics.stdev(BERT_Scores_1['recall'])\n",
    "            f1_mean_1 = statistics.mean(BERT_Scores_1['f1'])\n",
    "            f1_stdev_1 = statistics.stdev(BERT_Scores_1['f1'])\n",
    "            hash_code_1 = BERT_Scores_1['hashcode']\n",
    "        else:\n",
    "            test_bert_df = response_df_list[response_df_index][[Response_col,ModelSummary_col]].copy(deep=True)\n",
    "            test_bert_df[Response_col] = test_bert_df[Response_col].replace('', np.nan)\n",
    "            test_bert_df = test_bert_df.dropna(subset=[Response_col])\n",
    "            test_bert_df = test_bert_df.reset_index(drop=True)\n",
    "            \n",
    "            BERT_Scores_2 = bertscore.compute(predictions=test_bert_df[Response_col], references=test_bert_df[ModelSummary_col], lang=\"en\")\n",
    "            precision_mean_2 = statistics.mean(BERT_Scores_2['precision'])\n",
    "            precision_stdev_2 = statistics.stdev(BERT_Scores_2['precision'])\n",
    "            recall_mean_2 = statistics.mean(BERT_Scores_2['recall'])\n",
    "            recall_stdev_2 = statistics.stdev(BERT_Scores_2['recall'])\n",
    "            f1_mean_2 = statistics.mean(BERT_Scores_2['f1'])\n",
    "            f1_stdev_2 = statistics.stdev(BERT_Scores_2['f1'])\n",
    "            hash_code_2 = BERT_Scores_2['hashcode']\n",
    "        response_df_counter+=1\n",
    "        \n",
    "    \n",
    "    print('BERT_Scores_1 : ',BERT_Scores_1)\n",
    "    print('BERT_Scores_2 : ',BERT_Scores_2)\n",
    "    \n",
    "    ref_row = [technique_name , \n",
    "               (precision_mean_1 + precision_mean_2)/2,\n",
    "               (precision_stdev_1 + precision_stdev_2)/2,\n",
    "               (recall_mean_1 + recall_mean_2)/2,\n",
    "               (recall_stdev_1 + recall_stdev_2)/2,\n",
    "               (f1_mean_1 + f1_mean_2)/2,\n",
    "               (f1_stdev_1 + f1_stdev_2)/2,\n",
    "               hash_code_1\n",
    "               ]\n",
    "    \n",
    "    print('ref_row : ',ref_row)\n",
    "    \n",
    "    AllBERT_Scores.loc[-1] = ref_row\n",
    "    AllBERT_Scores.index = AllBERT_Scores.index + 1  #shift index\n",
    "    AllBERT_Scores = AllBERT_Scores.sort_index()  #sort by index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Engineering Technique</th>\n",
       "      <th>Precision Mean</th>\n",
       "      <th>Precision Stddev</th>\n",
       "      <th>Recall Mean</th>\n",
       "      <th>Recall Stddev</th>\n",
       "      <th>F1 Score Mean</th>\n",
       "      <th>F1 Score Stddev</th>\n",
       "      <th>hashcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SINGLE_0_96T_zero_shot_single_response_df</td>\n",
       "      <td>0.885445</td>\n",
       "      <td>0.024749</td>\n",
       "      <td>0.862300</td>\n",
       "      <td>0.021424</td>\n",
       "      <td>0.873681</td>\n",
       "      <td>0.022351</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SINGLE_0_64T_zero_shot_single_response_df</td>\n",
       "      <td>0.886623</td>\n",
       "      <td>0.017167</td>\n",
       "      <td>0.866169</td>\n",
       "      <td>0.013650</td>\n",
       "      <td>0.876246</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SINGLE_48topk_zero_shot_single_response_df</td>\n",
       "      <td>0.887272</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>0.865990</td>\n",
       "      <td>0.010636</td>\n",
       "      <td>0.876447</td>\n",
       "      <td>0.014518</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SINGLE_32topk_zero_shot_single_response_df</td>\n",
       "      <td>0.887839</td>\n",
       "      <td>0.019354</td>\n",
       "      <td>0.864531</td>\n",
       "      <td>0.011968</td>\n",
       "      <td>0.875982</td>\n",
       "      <td>0.014471</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SINGLE_1_08topp_zero_shot_single_response_df</td>\n",
       "      <td>0.888724</td>\n",
       "      <td>0.021692</td>\n",
       "      <td>0.866987</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>0.877669</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SINGLE_0_72topp_zero_shot_single_response_df</td>\n",
       "      <td>0.891511</td>\n",
       "      <td>0.020963</td>\n",
       "      <td>0.867143</td>\n",
       "      <td>0.014506</td>\n",
       "      <td>0.879098</td>\n",
       "      <td>0.016252</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Prompt Engineering Technique  Precision Mean  \\\n",
       "0     SINGLE_0_96T_zero_shot_single_response_df        0.885445   \n",
       "1     SINGLE_0_64T_zero_shot_single_response_df        0.886623   \n",
       "2    SINGLE_48topk_zero_shot_single_response_df        0.887272   \n",
       "3    SINGLE_32topk_zero_shot_single_response_df        0.887839   \n",
       "4  SINGLE_1_08topp_zero_shot_single_response_df        0.888724   \n",
       "5  SINGLE_0_72topp_zero_shot_single_response_df        0.891511   \n",
       "\n",
       "   Precision Stddev  Recall Mean  Recall Stddev  F1 Score Mean  \\\n",
       "0          0.024749     0.862300       0.021424       0.873681   \n",
       "1          0.017167     0.866169       0.013650       0.876246   \n",
       "2          0.020541     0.865990       0.010636       0.876447   \n",
       "3          0.019354     0.864531       0.011968       0.875982   \n",
       "4          0.021692     0.866987       0.016908       0.877669   \n",
       "5          0.020963     0.867143       0.014506       0.879098   \n",
       "\n",
       "   F1 Score Stddev                                           hashcode  \n",
       "0         0.022351  roberta-large_L17_no-idf_version=0.3.12(hug_tr...  \n",
       "1         0.014526  roberta-large_L17_no-idf_version=0.3.12(hug_tr...  \n",
       "2         0.014518  roberta-large_L17_no-idf_version=0.3.12(hug_tr...  \n",
       "3         0.014471  roberta-large_L17_no-idf_version=0.3.12(hug_tr...  \n",
       "4         0.018271  roberta-large_L17_no-idf_version=0.3.12(hug_tr...  \n",
       "5         0.016252  roberta-large_L17_no-idf_version=0.3.12(hug_tr...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AllBERT_Scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
